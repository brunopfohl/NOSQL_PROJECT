{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Category 1: Basic Data Operations\n",
    "This notebook demonstrates basic CRUD operations in a sharded environment:\n",
    "\n",
    "1. Insert with validation\n",
    "2. Bulk inserts with shard analysis\n",
    "3. Complex updates across shards\n",
    "4. Delete operations with shard awareness\n",
    "5. Upsert operations\n",
    "6. Atomic operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and Connection\n",
    "import sys\n",
    "!{sys.executable} -m pip install pandas pymongo --quiet\n",
    "\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def print_mongo(obj):\n",
    "    print(json.dumps(obj, indent=2, default=str))\n",
    "\n",
    "client = MongoClient('mongodb://admin:admin@router1:27017/businessdb?authSource=admin')\n",
    "db = client.businessdb\n",
    "print(\"Connected to MongoDB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Insert with Validation\n",
    "Task: Insert a new organization with validation rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted document with ID: 6771e2c5ffa110f4979adcbf\n"
     ]
    }
   ],
   "source": [
    "new_org = {\n",
    "    \"organizationId\": \"TEST123\",\n",
    "    \"name\": \"Test Company\",\n",
    "    \"industry\": \"Technology\",\n",
    "    \"country\": \"Czech Republic\",\n",
    "    \"founded\": 2023,\n",
    "    \"numberOfEmployees\": 100\n",
    "}\n",
    "\n",
    "try:\n",
    "    result = db.organizations.insert_one(new_org)\n",
    "    print(f\"Inserted document with ID: {result.inserted_id}\")\n",
    "except Exception as e:\n",
    "    print(f\"Validation error: {e}\")\n",
    "\n",
    "# Explanation: This command demonstrates document insertion with schema validation\n",
    "# The document must match the schema defined in init-collections.js"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Bulk Insert with Shard Distribution Analysis\n",
    "Task: Insert organizations and analyze how data is distributed across shards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bulk_orgs = [\n",
    "    {\n",
    "        \"organizationId\": f\"ORG{i}\",\n",
    "        \"name\": f\"Company {i}\",\n",
    "        \"industry\": \"Technology\",\n",
    "        \"country\": \"Country {i % 5}\",\n",
    "        \"founded\": 2000 + (i % 20),\n",
    "        \"numberOfEmployees\": 50 + (i * 10)\n",
    "    }\n",
    "    for i in range(100)\n",
    "]\n",
    "\n",
    "try:\n",
    "    result = db.organizations.insert_many(bulk_orgs)\n",
    "    print(f\"Inserted {len(result.inserted_ids)} documents\")\n",
    "except Exception as e:\n",
    "    print(f\"Bulk insert error: {e}\")\n",
    "\n",
    "shard_stats = db.command('shardCollection', 'businessdb.organizations', key={'organizationId': 1})\n",
    "print(\"Distribution for organizations:\")\n",
    "for shard, stats in shard_stats['shards'].items():\n",
    "    print(f\"Shard {shard}: {stats['count']} documents, {stats['size']}MB\")\n",
    "\n",
    "print(\"Detailed stats:\")\n",
    "print_mongo(db.organizations.stats())\n",
    "\n",
    "# Explanation: This command demonstrates bulk insertion and shard distribution analysis\n",
    "# The data is inserted and then analyzed to see how it is distributed across shards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Advanced Upsert with Version Tracking\n",
    "Task: Update or insert documents with automatic version control across shards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def upsert_with_version_tracking(org_id, data):\n",
    "    # Start session for consistency\n",
    "    with client.start_session() as session:\n",
    "        # Get current version if exists\n",
    "        current = db.organizations.find_one(\n",
    "            {\"organizationId\": org_id},\n",
    "            session=session\n",
    "        )\n",
    "        \n",
    "        # Prepare version metadata\n",
    "        version_data = {\n",
    "            \"version\": (current.get(\"version\", 0) + 1 if current else 1),\n",
    "            \"lastModified\": datetime.utcnow(),\n",
    "            \"modifiedBy\": \"system\"\n",
    "        }\n",
    "        \n",
    "        # Combine with new data\n",
    "        update_data = {**data, **version_data}\n",
    "        \n",
    "        # Perform upsert\n",
    "        result = db.organizations.update_one(\n",
    "            {\"organizationId\": org_id},\n",
    "            {\"$set\": update_data},\n",
    "            upsert=True,\n",
    "            session=session\n",
    "        )\n",
    "        \n",
    "        # Log version history\n",
    "        if current:\n",
    "            db.organizations_versions.insert_one(\n",
    "                {**current, \"replacedAt\": datetime.utcnow()},\n",
    "                session=session\n",
    "            )\n",
    "            \n",
    "        return result\n",
    "\n",
    "# Example usage\n",
    "result = upsert_with_version_tracking(\n",
    "    \"ORG_TEST_1\",\n",
    "    {\n",
    "        \"name\": \"Test Organization\",\n",
    "        \"industry\": \"Technology\",\n",
    "        \"status\": \"active\"\n",
    "    }\n",
    ")\n",
    "print(\"Operation result:\", result.modified_count, \"modified,\", \"new document created\" if result.upserted_id else \"existing document updated\")\n",
    "\n",
    "# Verify versions\n",
    "versions = list(db.organizations_versions.find({\"organizationId\": \"ORG_TEST_1\"}))\n",
    "print(f\"\\nVersion history: {len(versions)} previous versions\")\n",
    "for v in versions:\n",
    "    print(f\"Version {v['version']} from {v['lastModified']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Complex Document History System\n",
    "Task: Implement a sophisticated versioning system with diff tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepdiff import DeepDiff\n",
    "\n",
    "class DocumentTracker:\n",
    "    def __init__(self, collection, history_collection):\n",
    "        self.collection = collection\n",
    "        self.history = history_collection\n",
    "    \n",
    "    def update_with_history(self, filter_query, updates, metadata=None):\n",
    "        with client.start_session() as session:\n",
    "            # Get current state\n",
    "            current_doc = self.collection.find_one(filter_query)\n",
    "            \n",
    "            if not current_doc:\n",
    "                raise ValueError(\"Document not found\")\n",
    "            \n",
    "            # Calculate differences\n",
    "            diff = DeepDiff(current_doc, updates, ignore_order=True)\n",
    "            \n",
    "            # Create history record\n",
    "            history_record = {\n",
    "                \"documentId\": current_doc[\"_id\"],\n",
    "                \"timestamp\": datetime.utcnow(),\n",
    "                \"previousVersion\": current_doc,\n",
    "                \"changes\": diff.to_dict(),\n",
    "                \"metadata\": metadata or {}\n",
    "            }\n",
    "            \n",
    "            # Update document and store history atomically\n",
    "            session.start_transaction()\n",
    "            try:\n",
    "                self.collection.update_one(\n",
    "                    filter_query,\n",
    "                    {\"$set\": updates},\n",
    "                    session=session\n",
    "                )\n",
    "                self.history.insert_one(history_record, session=session)\n",
    "                session.commit_transaction()\n",
    "                return True\n",
    "            except Exception as e:\n",
    "                session.abort_transaction()\n",
    "                raise e\n",
    "\n",
    "# Initialize tracker\n",
    "tracker = DocumentTracker(db.organizations, db.organizations_history)\n",
    "\n",
    "# Example usage\n",
    "try:\n",
    "    tracker.update_with_history(\n",
    "        {\"organizationId\": \"ORG_TEST_1\"},\n",
    "        {\"status\": \"inactive\", \"lastUpdateReason\": \"Company restructuring\"},\n",
    "        metadata={\"updatedBy\": \"admin\", \"reason\": \"status change\"}\n",
    "    )\n",
    "    print(\"Update successful with history tracking\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5: Cross-Shard Atomic Operations\n",
    "Task: Implement complex atomic operations across multiple shards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atomic_multi_org_operation(operations):\n",
    "    \"\"\"Execute multiple operations across different shards atomically\"\"\"\n",
    "    with client.start_session() as session:\n",
    "        session.start_transaction()\n",
    "        try:\n",
    "            results = []\n",
    "            for op in operations:\n",
    "                if op['type'] == 'update':\n",
    "                    result = db.organizations.update_one(\n",
    "                        op['filter'],\n",
    "                        op['update'],\n",
    "                        session=session\n",
    "                    )\n",
    "                elif op['type'] == 'insert':\n",
    "                    result = db.organizations.insert_one(\n",
    "                        op['document'],\n",
    "                        session=session\n",
    "                    )\n",
    "                results.append({\n",
    "                    'operation': op['type'],\n",
    "                    'success': True,\n",
    "                    'result': result\n",
    "                })\n",
    "            \n",
    "            session.commit_transaction()\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            session.abort_transaction()\n",
    "            raise Exception(f\"Transaction failed: {str(e)}\")\n",
    "\n",
    "# Example usage\n",
    "ops = [\n",
    "    {\n",
    "        'type': 'update',\n",
    "        'filter': {'organizationId': 'ORG_TEST_1'},\n",
    "        'update': {'$set': {'status': 'merging'}}\n",
    "    },\n",
    "    {\n",
    "        'type': 'insert',\n",
    "        'document': {\n",
    "            'organizationId': 'ORG_TEST_2',\n",
    "            'name': 'Test Org 2',\n",
    "            'status': 'active'\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "try:\n",
    "    results = atomic_multi_org_operation(ops)\n",
    "    print(\"All operations completed successfully\")\n",
    "    for r in results:\n",
    "        print(f\"{r['operation']}: {r['result']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Transaction failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 6: Parallel Bulk Operations\n",
    "Task: Execute bulk operations with parallel processing and shard awareness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pymongo import UpdateOne, InsertOne\n",
    "import threading\n",
    "\n",
    "class ShardedBulkOperator:\n",
    "    def __init__(self, collection, batch_size=100, max_workers=4):\n",
    "        self.collection = collection\n",
    "        self.batch_size = batch_size\n",
    "        self.max_workers = max_workers\n",
    "        self._local = threading.local()\n",
    "    \n",
    "    def _get_session(self):\n",
    "        if not hasattr(self._local, 'session'):\n",
    "            self._local.session = client.start_session()\n",
    "        return self._local.session\n",
    "    \n",
    "    def _process_batch(self, operations):\n",
    "        session = self._get_session()\n",
    "        try:\n",
    "            result = self.collection.bulk_write(\n",
    "                operations,\n",
    "                session=session,\n",
    "                ordered=False\n",
    "            )\n",
    "            return {\n",
    "                'inserted': result.inserted_count,\n",
    "                'modified': result.modified_count,\n",
    "                'deleted': result.deleted_count\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Batch error: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def execute(self, operations):\n",
    "        # Split into batches\n",
    "        batches = [operations[i:i + self.batch_size] \n",
    "                   for i in range(0, len(operations), self.batch_size)]\n",
    "        \n",
    "        results = []\n",
    "        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "            futures = [executor.submit(self._process_batch, batch) \n",
    "                      for batch in batches]\n",
    "            \n",
    "            for future in futures:\n",
    "                try:\n",
    "                    results.append(future.result())\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed batch: {e}\")\n",
    "        \n",
    "        return self._aggregate_results(results)\n",
    "    \n",
    "    def _aggregate_results(self, results):\n",
    "        totals = {'inserted': 0, 'modified': 0, 'deleted': 0}\n",
    "        for r in results:\n",
    "            for k in totals:\n",
    "                totals[k] += r.get(k, 0)\n",
    "        return totals\n",
    "\n",
    "# Example usage\n",
    "bulk_ops = []\n",
    "for i in range(1000):\n",
    "    if i % 2 == 0:\n",
    "        bulk_ops.append(InsertOne({\n",
    "            'organizationId': f'BULK_{i}',\n",
    "            'name': f'Bulk Organization {i}',\n",
    "            'status': 'active'\n",
    "        }))\n",
    "    else:\n",
    "        bulk_ops.append(UpdateOne(\n",
    "            {'organizationId': f'BULK_{i-1}'},\n",
    "            {'$set': {'status': 'updated'}}\n",
    "        ))\n",
    "\n",
    "operator = ShardedBulkOperator(db.organizations)\n",
    "results = operator.execute(bulk_ops)\n",
    "print(\"Bulk operation results:\")\n",
    "print(f\"Inserted: {results['inserted']}\")\n",
    "print(f\"Modified: {results['modified']}\")\n",
    "print(f\"Deleted: {results['deleted']}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
